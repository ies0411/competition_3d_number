{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5c1debf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py  # .h5 파일을 읽기 위한 패키지\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# Convnet, ensenble, smoothing, baysian search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50509c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcb49651",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS': 300,\n",
    "    'LEARNING_RATE': 1e-3,\n",
    "    'BATCH_SIZE': 12,\n",
    "    #     'BATCH_SIZE': 80,\n",
    "\n",
    "    'SEED': 41\n",
    "}\n",
    "aug_ratio = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2e2c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything(CFG['SEED'])  # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddad6595",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv('./open/train.csv')\n",
    "all_points = h5py.File('./open/train.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bae4cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from volumentations import *\n",
    "\n",
    "\n",
    "def get_augmentation(patch_size):\n",
    "    return Compose([\n",
    "        Rotate((-90, 90), (0, 0), (0, 0), p=0.5),\n",
    "        Rotate((0, 0), (-90, 90), (-90, 90), p=0.5),\n",
    "        Rotate((-15, 15), (-15, 15), (-15, 15), p=0.5),\n",
    "        Rotate((0, 0), (0, 0), (-90, 90), p=0.5),\n",
    "        RandomCropFromBorders(crop_value=0.1, p=0.5),\n",
    "        ElasticTransform((0, 0.25), interpolation=2, p=0.5),\n",
    "        Resize((100, 100, 100), interpolation=1,\n",
    "               resize_type=0, always_apply=True, p=0.5),\n",
    "        Flip(0, p=0.5),\n",
    "        Flip(1, p=0.5),\n",
    "        Flip(2, p=0.5),\n",
    "        RandomRotate90(p=0.5),\n",
    "        #         RandomRotate90((1, 2), p=0.7),\n",
    "        #         RandomScale(p=0.7),\n",
    "        Downscale(p=0.5),\n",
    "        GaussianNoise(var_limit=(0, 5), p=0.5),\n",
    "        #         RandomGamma(gamma_limit=(80, 120), p=0.2),\n",
    "        #                 Normalize()\n",
    "    ], p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9e6053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, id_list, label_list, point_list, transform=False):\n",
    "        self.id_list = id_list\n",
    "        self.label_list = label_list\n",
    "        self.point_list = point_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.id_list[index]\n",
    "\n",
    "        # h5파일을 바로 접근하여 사용하면 학습 속도가 병목 현상으로 많이 느릴 수 있습니다.\n",
    "        points = self.point_list[str(image_id)][:]\n",
    "        image = self.get_vector(points)\n",
    "        # TODO : transform\n",
    "        if self.transform is True:\n",
    "            #             print(\"transform\")\n",
    "            data = {'image': image}\n",
    "            aug = get_augmentation((80, 80, 80))\n",
    "            aug_data = aug(**data)\n",
    "            img = aug_data['image']\n",
    "            if self.label_list is not None:\n",
    "                label = self.label_list[index]\n",
    "                return torch.Tensor(img).unsqueeze(0), label\n",
    "            else:\n",
    "                return torch.Tensor(img).unsqueeze(0)\n",
    "\n",
    "        else:\n",
    "            if self.label_list is True:\n",
    "                label = self.label_list[index]\n",
    "                return torch.Tensor(image).unsqueeze(0), label\n",
    "            else:\n",
    "                return torch.Tensor(image).unsqueeze(0)\n",
    "\n",
    "    def get_vector(self, points, x_y_z=[80, 80, 80]):\n",
    "        # 3D Points -> [16,16,16]\n",
    "        xyzmin = np.min(points, axis=0) - 0.001\n",
    "        xyzmax = np.max(points, axis=0) + 0.001\n",
    "\n",
    "        diff = max(xyzmax-xyzmin) - (xyzmax-xyzmin)\n",
    "        xyzmin = xyzmin - diff / 2\n",
    "        xyzmax = xyzmax + diff / 2\n",
    "\n",
    "        segments = []\n",
    "        shape = []\n",
    "\n",
    "        for i in range(3):\n",
    "            # note the +1 in num\n",
    "            if type(x_y_z[i]) is not int:\n",
    "                raise TypeError(\"x_y_z[{}] must be int\".format(i))\n",
    "            s, step = np.linspace(\n",
    "                xyzmin[i], xyzmax[i], num=(x_y_z[i] + 1), retstep=True)\n",
    "            segments.append(s)\n",
    "            shape.append(step)\n",
    "\n",
    "        n_voxels = x_y_z[0] * x_y_z[1] * x_y_z[2]\n",
    "        n_x = x_y_z[0]\n",
    "        n_y = x_y_z[1]\n",
    "        n_z = x_y_z[2]\n",
    "\n",
    "        structure = np.zeros((len(points), 4), dtype=int)\n",
    "        structure[:, 0] = np.searchsorted(segments[0], points[:, 0]) - 1\n",
    "        structure[:, 1] = np.searchsorted(segments[1], points[:, 1]) - 1\n",
    "        structure[:, 2] = np.searchsorted(segments[2], points[:, 2]) - 1\n",
    "\n",
    "        # i = ((y * n_x) + x) + (z * (n_x * n_y))\n",
    "        structure[:, 3] = ((structure[:, 1] * n_x) +\n",
    "                           structure[:, 0]) + (structure[:, 2] * (n_x * n_y))\n",
    "\n",
    "        vector = np.zeros(n_voxels)\n",
    "        count = np.bincount(structure[:, 3])\n",
    "        vector[:len(count)] = count\n",
    "\n",
    "        vector = vector.reshape(n_z, n_y, n_x)\n",
    "#         print(vector.shape)\n",
    "        return vector\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "596f3230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install monai\n",
    "# !pip install monai\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"./3D-CNN-PyTorch/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4eeaed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/shijianjian/EfficientNet-PyTorch-3D\n",
    "# !pip install git+https://github.com/shijianjian/EfficientNet-PyTorch-3D\n",
    "from torch import nn\n",
    "import torch\n",
    "from models.cnn import cnn3d\n",
    "from models import (cnn, C3DNet, resnet, ResNetV2, ResNeXt, ResNeXtV2, WideResNet, PreActResNet,\n",
    "                    EfficientNet, DenseNet, ShuffleNet, ShuffleNetV2, SqueezeNet, MobileNet, MobileNetV2)\n",
    "from opts import parse_opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a22a4211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch_3d import EfficientNet3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae0cd033",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WideResNet\n",
    "model_depth = [50, 101, 152, 200]\n",
    "\"\"\"\n",
    "# model = ResNeXt.generate_model(\n",
    "#     model_depth=101,\n",
    "#     n_classes=10,\n",
    "#     in_channels=1,\n",
    "#     sample_size=128,\n",
    "#     sample_duration=16)\n",
    "\n",
    "\"\"\"\n",
    " 3D resnet\n",
    " model_depth = [10, 18, 34, 50, 101, 152, 200]\n",
    " \"\"\"\n",
    "# model = resnet.generate_model(\n",
    "#     model_depth=10,\n",
    "#     n_classes=10,\n",
    "#     n_input_channels=1,\n",
    "#     shortcut_type='B',\n",
    "#     conv1_t_size=7,\n",
    "#     conv1_t_stride=1,\n",
    "#     no_max_pool=False,\n",
    "#     widen_factor=1.0)\n",
    "\n",
    "\"\"\"\n",
    "3D resnet\n",
    "model_depth = [10, 18, 34, 50, 101, 152, 200]\n",
    "\"\"\"\n",
    "# model = ResNetV2.generate_model(\n",
    "#     model_depth=50,\n",
    "#     n_classes=10,\n",
    "#     n_input_channels=1,\n",
    "#     shortcut_type='B',\n",
    "#     conv1_t_size=7,\n",
    "#     conv1_t_stride=1,\n",
    "#     no_max_pool=False,\n",
    "#     widen_factor=1.0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "3D resnet\n",
    "model_depth = [121, 169, 201]\n",
    "\"\"\"\n",
    "# model = DenseNet.generate_model(\n",
    "#     model_depth=121,\n",
    "#     num_classes=10,\n",
    "#     n_input_channels=1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "EfficientNet\n",
    "\"\"\"\n",
    "model = EfficientNet3D.from_name(\n",
    "    \"efficientnet-b4\", override_params={'num_classes': 10}, in_channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b89a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device, epoch, best_score):\n",
    "    model.to(device)\n",
    "#     model.train()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    best_score = 0\n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for data, label in tqdm(iter(train_loader)):\n",
    "            data, label = data.float().to(device), label.long().to(device)\n",
    "            optimizer.zero_grad()\n",
    "#             print(data.shape)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             scheduler.step()\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        val_loss, val_acc = validation(model, criterion, val_loader, device)\n",
    "        print(\n",
    "            f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss)}] Val Loss : [{val_loss}] Val ACC : [{val_acc}]')\n",
    "\n",
    "        if best_score < val_acc:\n",
    "            best_score = val_acc\n",
    "            torch.save(model.state_dict(),\n",
    "                       './best_model_effi_b4_kfold_aug.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "795fe5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    model_preds = []\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for data, label in tqdm(iter(val_loader)):\n",
    "            data, label = data.float().to(device), label.long().to(device)\n",
    "\n",
    "            model_pred = model(data)\n",
    "            loss = criterion(model_pred, label)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            model_preds += model_pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "            true_labels += label.detach().cpu().numpy().tolist()\n",
    "\n",
    "    return np.mean(val_loss), accuracy_score(true_labels, model_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a2dbe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c824f96e38043db977a21f56e1a71e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd11681000834ff2b9162e6a54f49c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [1] Train Loss : [2.3855204547888946] Val Loss : [2.0804184003413724] Val ACC : [0.2746]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa49b0f138048c0880b74310a821a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3651c437e2416a8b6dc40b915d528e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [2] Train Loss : [2.0551196164761607] Val Loss : [1.93878174185467] Val ACC : [0.2994]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb792a3819c4211a1c10fefa904b728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1234074dd826426ca25684133107c890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [3] Train Loss : [1.9760235448261185] Val Loss : [1.825211326495635] Val ACC : [0.3622]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545c112d58014b2b81926345578c2f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6,9이면 augment flip제외\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "num_folds = 5\n",
    "\n",
    "epoch = 0\n",
    "checkpoint = torch.load('./best_model_effi_b4_kfold_aug.pth')\n",
    "model.load_state_dict(checkpoint)\n",
    "best_score = 0\n",
    "while epoch < CFG['EPOCHS']:\n",
    "    #     if epoch > 0:\n",
    "    #         aug_ratio = 0.2\n",
    "    #     elif epoch > 1:\n",
    "    #         aug_ratio = 0.5\n",
    "    #     elif epoch > 2:\n",
    "    #         aug_ratio = 0.7\n",
    "    #     elif epoch > 3:\n",
    "    #         aug_ratio = 1.0\n",
    "    splits = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(all_df)))):\n",
    "\n",
    "        train_df = all_df.iloc[train_idx, :]\n",
    "        val_df = all_df.iloc[val_idx, :]\n",
    "\n",
    "        train_dataset = CustomDataset(\n",
    "            train_df['ID'].values, train_df['label'].values, all_points, transform=True)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "        val_dataset = CustomDataset(\n",
    "            val_df['ID'].values, val_df['label'].values, all_points, transform=True)\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "#         optimizer = torch.optim.Adam(\n",
    "#             model.parameters(), lr=0.0004)\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(), lr=0.0001, momentum=0.9)\n",
    "#             scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps = len(train_loader)*5,num_training_steps = len(train_loader)*CFG['EPOCHS'])\n",
    "        train(model, optimizer, train_loader, val_loader,\n",
    "              None, device, epoch, best_score)\n",
    "        epoch += 1\n",
    "        if epoch == (CFG['EPOCHS']+1):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a17484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./open/sample_submission.csv')\n",
    "test_points = h5py.File('./open/test.h5', 'r')\n",
    "\n",
    "# self, id_list, label_list, point_list, transform = None):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2af908",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(\n",
    "    test_df['ID'].values, label_list=None, point_list=test_points, transform=True)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28b5e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    model_preds = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(iter(test_loader)):\n",
    "            data = data.float().to(device)\n",
    "\n",
    "            batch_pred = model(data)\n",
    "\n",
    "            model_preds += torch.softmax(batch_pred.cpu(),\n",
    "                                         dim=1).numpy().tolist()\n",
    "\n",
    "    return model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3dad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e41ae78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 10)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_preds = np.array(preds)\n",
    "np_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dacb23ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee614f7aa78494ebf74eb91d26a9824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f9ba1b5c62478cb6745502bade23ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9fb00049c24b98a46537e41364058c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c39b2bb60b4c28a32cf389533f64ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afff303f4242497c878a86818c3a99d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b32704579d046d483f2f5a3cf2c23b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5f6a44c5204646bf5a901d5e450efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c195f661ae2d439e8a23e4ecc7eb5f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39aa4d4c09b9434b876c9eca24562eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aae400709674d61918fe003a90bacfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_TTA = 10\n",
    "preds_tta = []\n",
    "for i in range(num_TTA):\n",
    "    preds = predict(model, test_loader, device)\n",
    "    preds_tta += preds\n",
    "# preds_TTA = np.zeros((lesn(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7bb4da1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 10)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_tta_np = np.array(preds_tta)\n",
    "preds_tta_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "730aa386",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_tta_np = preds_tta_np.reshape(-1, 40000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "59ee65d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 40000, 10)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_tta_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "623ae252",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_sum = 0.0\n",
    "for i in range(10):\n",
    "    preds_sum += preds_tta_np[i]\n",
    "preds_mean = preds_sum/10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1dd5dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_mean = preds_mean.argmax(1)\n",
    "#             model_preds += batch_pred.argmax(1).detach().cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ba600f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 4, 2, ..., 4, 5, 7])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a204d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['label'] = sub_mean\n",
    "test_df.to_csv('./submit_effi_kfold_tta.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b766e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603bc782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9995148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(\n",
    "    test_df['ID'].values, label_list=None, point_list=test_points, transform=True)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
